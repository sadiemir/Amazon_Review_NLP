{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ffb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694d6e8",
   "metadata": {},
   "source": [
    "#### Step 1: Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ce7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed train data\n",
    "df_train_preprocessed= pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40a5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed test data\n",
    "df_test_preprocessed = pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d59e7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>['im', 'reading', 'lot', 'review', 'saying', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>['soundtrack', 'favorite', 'music', 'time', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>['truly', 'like', 'soundtrack', 'enjoy', 'vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>['youve', 'played', 'game', 'know', 'divine', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>['quite', 'sure', 'actually', 'taking', 'time'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title   \n",
       "0         2              The best soundtrack ever to anything.  \\\n",
       "1         2                                           Amazing!   \n",
       "2         2                               Excellent Soundtrack   \n",
       "3         2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "4         2                            an absolute masterpiece   \n",
       "\n",
       "                                                text  \n",
       "0  ['im', 'reading', 'lot', 'review', 'saying', '...  \n",
       "1  ['soundtrack', 'favorite', 'music', 'time', 'h...  \n",
       "2  ['truly', 'like', 'soundtrack', 'enjoy', 'vide...  \n",
       "3  ['youve', 'played', 'game', 'know', 'divine', ...  \n",
       "4  ['quite', 'sure', 'actually', 'taking', 'time'...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0568f966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>['despite', 'fact', 'played', 'small', 'portio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>['bought', 'charger', 'jul', '2003', 'worked',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>['check', 'maha', 'energy', 'website', 'powere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>['reviewed', 'quite', 'bit', 'combo', 'player'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>['also', 'began', 'incorrect', 'disc', 'proble...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title   \n",
       "0         2  One of the best game music soundtracks - for a...  \\\n",
       "1         1                   Batteries died within a year ...   \n",
       "2         2              works fine, but Maha Energy is better   \n",
       "3         2                       Great for the non-audiophile   \n",
       "4         1              DVD Player crapped out after one year   \n",
       "\n",
       "                                                text  \n",
       "0  ['despite', 'fact', 'played', 'small', 'portio...  \n",
       "1  ['bought', 'charger', 'jul', '2003', 'worked',...  \n",
       "2  ['check', 'maha', 'energy', 'website', 'powere...  \n",
       "3  ['reviewed', 'quite', 'bit', 'combo', 'player'...  \n",
       "4  ['also', 'began', 'incorrect', 'disc', 'proble...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f0b12",
   "metadata": {},
   "source": [
    "#### Step 2: Convert text to numerical features. \n",
    "Options are: Bag of word (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Embeddings (Optional for Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3060bce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#convert strings in train and test set to list on column 'text'\n",
    "\n",
    "df_train_preprocessed['text'] = df_train_preprocessed['text'].apply(lambda x: eval(x))\n",
    "df_test_preprocessed['text'] = df_test_preprocessed['text'].apply(lambda x: eval(x))\n",
    "\n",
    "#check the type of column 'text'\n",
    "print(type(df_train_preprocessed['text'].iloc[0]))\n",
    "print(type(df_test_preprocessed['text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea77dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i m   r e a d i n g   l o t   r e v i e w   s ...\n",
      "1    s o u n d t r a c k   f a v o r i t e   m u s ...\n",
      "2    t r u l y   l i k e   s o u n d t r a c k   e ...\n",
      "3    y o u v e   p l a y e d   g a m e   k n o w   ...\n",
      "4    q u i t e   s u r e   a c t u a l l y   t a k ...\n",
      "Name: text, dtype: object\n",
      "0    d e s p i t e   f a c t   p l a y e d   s m a ...\n",
      "1    b o u g h t   c h a r g e r   j u l   2 0 0 3 ...\n",
      "2    c h e c k   m a h a   e n e r g y   w e b s i ...\n",
      "3    r e v i e w e d   q u i t e   b i t   c o m b ...\n",
      "4    a l s o   b e g a n   i n c o r r e c t   d i ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#join the tokenized words into a single string for each row\n",
    "df_train_preprocessed['text'] = df_train_preprocessed['text'].apply(lambda x: ' '.join(x))\n",
    "df_test_preprocessed['text'] = df_test_preprocessed['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#verify the joined text\n",
    "print(df_train_preprocessed['text'].head())\n",
    "print(df_test_preprocessed['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08064634",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Fit and transform the train data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_preprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Transform the test data using the same vocabulary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_test \u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mtransform(df_test_preprocessed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#Start with the simplest model, Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Fit and transform the train data\n",
    "X_train = vectorizer.fit_transform(df_train_preprocessed['text'])\n",
    "\n",
    "#Transform the test data using the same vocabulary\n",
    "X_test =vectorizer.transform(df_test_preprocessed['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b29018",
   "metadata": {},
   "source": [
    "Step 3: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c64d63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8745421863554659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.87      0.87    200000\n",
      "           2       0.87      0.88      0.88    199999\n",
      "\n",
      "    accuracy                           0.87    399999\n",
      "   macro avg       0.87      0.87      0.87    399999\n",
      "weighted avg       0.87      0.87      0.87    399999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, df_train_preprocessed['polarity'])\n",
    "\n",
    "#make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(df_test_preprocessed['polarity'], y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#Print a classification report for detailed evaluation\n",
    "print(classification_report(df_test_preprocessed['polarity'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ae12e",
   "metadata": {},
   "source": [
    "#### Time for a more advanced model to train this dataset like Deep Learning Models for NLP: RNNs, LSTMs, and GRUs\n",
    "- Vanilla RNN: Use for tasks with short sequences or when computational efficiency is critical.\n",
    "- LSTM: Use for tasks with longer sequences, where retaining long-term dependencies is crucial (e.g., machine translation, long text classification).\n",
    "- GRU: Use when you want a compromise between LSTM’s performance and RNN’s efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa7186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data again: \n",
    "#the preprocessed train data\n",
    "df_train_preprocessed= pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57fc8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed test data\n",
    "df_test_preprocessed = pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7056b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "\n",
    "#LSTM model \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128)) # Embedding layer for word vector\n",
    "model.add(LSTM(units=128, return_sequences=True)) #LSTM layer\n",
    "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid')) #output layer for binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d721790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "#step 1: fit a tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000) #choose vocab size based on dataset size\n",
    "tokenizer.fit_on_texts(df_train_preprocessed['text']) #fit on the training data\n",
    "\n",
    "#Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(df_train_preprocessed['text'])\n",
    "X_test = tokenizer.texts_to_sequences(df_test_preprocessed['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6210387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: pad the sequence\n",
    "#LSTM model expects sequences to be of the same length, so padding is neccesary\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#pad sequences to ensure they are the same length\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3aba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine y_train and y_test\n",
    "y_train = df_train_preprocessed['polarity']\n",
    "y_test = df_test_preprocessed['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd774737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: train the LSTM model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=30, #set number of epochs\n",
    "    batch_size=32, #set batch size (32 and 64 are the best but in lack of good memory any number that is power of 2 works)\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e8231",
   "metadata": {},
   "source": [
    "It looks like the accuracy of the model is low. Considering that the data is balanced, this low accuracy can be for \n",
    "many reasons, such as improper preprocessing, overfitting/underfitting, incorrect labeling, inappropriate hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab3e8d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccae570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
