{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbf361",
   "metadata": {},
   "source": [
    "#### Step 1: Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6386c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "# we will use the preprocessed data from now on: \n",
    "df_train_preprocessed= pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d0b2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed = pd.read_csv('/Users/smirghor/Library/Mobile Documents/com~apple~CloudDocs/Personal/Machine Learning Projects/data.nosync/amazon_review_polarity_csv/preprocessed_reviews_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d3e5f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>['im', 'reading', 'lot', 'review', 'saying', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>['soundtrack', 'favorite', 'music', 'time', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>['truly', 'like', 'soundtrack', 'enjoy', 'vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>['youve', 'played', 'game', 'know', 'divine', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>['quite', 'sure', 'actually', 'taking', 'time'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title   \n",
       "0         2              The best soundtrack ever to anything.  \\\n",
       "1         2                                           Amazing!   \n",
       "2         2                               Excellent Soundtrack   \n",
       "3         2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "4         2                            an absolute masterpiece   \n",
       "\n",
       "                                                text  \n",
       "0  ['im', 'reading', 'lot', 'review', 'saying', '...  \n",
       "1  ['soundtrack', 'favorite', 'music', 'time', 'h...  \n",
       "2  ['truly', 'like', 'soundtrack', 'enjoy', 'vide...  \n",
       "3  ['youve', 'played', 'game', 'know', 'divine', ...  \n",
       "4  ['quite', 'sure', 'actually', 'taking', 'time'...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b6f7a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>['despite', 'fact', 'played', 'small', 'portio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>['bought', 'charger', 'jul', '2003', 'worked',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>['check', 'maha', 'energy', 'website', 'powere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>['reviewed', 'quite', 'bit', 'combo', 'player'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>['also', 'began', 'incorrect', 'disc', 'proble...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title   \n",
       "0         2  One of the best game music soundtracks - for a...  \\\n",
       "1         1                   Batteries died within a year ...   \n",
       "2         2              works fine, but Maha Energy is better   \n",
       "3         2                       Great for the non-audiophile   \n",
       "4         1              DVD Player crapped out after one year   \n",
       "\n",
       "                                                text  \n",
       "0  ['despite', 'fact', 'played', 'small', 'portio...  \n",
       "1  ['bought', 'charger', 'jul', '2003', 'worked',...  \n",
       "2  ['check', 'maha', 'energy', 'website', 'powere...  \n",
       "3  ['reviewed', 'quite', 'bit', 'combo', 'player'...  \n",
       "4  ['also', 'began', 'incorrect', 'disc', 'proble...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c24cd",
   "metadata": {},
   "source": [
    "#### Step 2: Convert text to numerical features. \n",
    "Options are: Bag of word (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Embeddings (Optional for Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "746c01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [ ' i m ' ,   ' r e a d i n g ' ,   ' l o t ' ...\n",
      "1    [ ' s o u n d t r a c k ' ,   ' f a v o r i t ...\n",
      "2    [ ' t r u l y ' ,   ' l i k e ' ,   ' s o u n ...\n",
      "3    [ ' y o u v e ' ,   ' p l a y e d ' ,   ' g a ...\n",
      "4    [ ' q u i t e ' ,   ' s u r e ' ,   ' a c t u ...\n",
      "Name: text, dtype: object\n",
      "0    [ ' d e s p i t e ' ,   ' f a c t ' ,   ' p l ...\n",
      "1    [ ' b o u g h t ' ,   ' c h a r g e r ' ,   ' ...\n",
      "2    [ ' c h e c k ' ,   ' m a h a ' ,   ' e n e r ...\n",
      "3    [ ' r e v i e w e d ' ,   ' q u i t e ' ,   ' ...\n",
      "4    [ ' a l s o ' ,   ' b e g a n ' ,   ' i n c o ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#join the tokenized words into a single string for each row\n",
    "df_train_preprocessed['text'] = df_train_preprocessed['text'].apply(lambda x: ' '.join(x))\n",
    "df_test_preprocessed['text'] = df_test_preprocessed['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#verify the joined text\n",
    "print(df_train_preprocessed['text'].head())\n",
    "print(df_test_preprocessed['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9637f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Fit and transform the train data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_preprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Transform the test data using the same vocabulary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_test \u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mtransform(df_test_preprocessed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x)))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#Start with the simplest model, Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Fit and transform the train data\n",
    "X_train = vectorizer.fit_transform(df_train_preprocessed['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "#Transform the test data using the same vocabulary\n",
    "X_test =vectorizer.transform(df_test_preprocessed['text'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad0f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'test': 5, 'example': 1, 'sentence': 4, 'text': 6, 'data': 0, 'machine': 3, 'learning': 2}\n"
     ]
    }
   ],
   "source": [
    "sample_data = ['This is a test', 'Another example sentence', 'Text data for machine learning']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_sample = vectorizer.fit_transform(sample_data)\n",
    "print(f\"Vocabulary: {vectorizer.vocabulary_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
